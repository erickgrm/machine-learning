{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import own library with custom technical routines\n",
    "from custom_routines import custom_routines as cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1  2\n",
      "0   3   4  1\n",
      "1   7   8  1\n",
      "2  40  10  1\n",
      "3  16  17  1\n",
      "4   8  18  1 319\n",
      "     0    0   1  2\n",
      "0  1.0    3   4  1\n",
      "1  1.0    7   8  1\n",
      "2  1.0   40  10  1\n",
      "3  1.0   16  17  1\n",
      "4  1.0    8  18  1\n",
      "5  1.0  112  40  0\n",
      "6  1.0   20  17  0\n",
      "7  1.0   21  22  1\n",
      "8  1.0   17  23  1\n",
      "9  1.0   24  12  0 319\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"~/Desktop/mcc20192/mlearning/tareas/tarea3/data/juegos_entrenamiento.txt\", header=None, delim_whitespace=True)\n",
    "print(data.head(5),len(data))\n",
    "data.insert(0,0,list(np.ones(len(data))), True)  \n",
    "print(data.head(10),len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot-encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(categories = 'auto')\n",
    "enc.fit(data)\n",
    "data_enc = enc.transform(data).toarray()\n",
    "data_enc\n",
    "#inv = enc.inverse_transform(data_enc)\n",
    "#inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(data.iloc[0:,0],'ko')\n",
    "#plt.plot(data.iloc[0:,1],'bs')\n",
    "\n",
    "#df = pd.DataFrame(data_enc)\n",
    "#df.hist(grid=False,bins=10,figsize=(14,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training an Bernoulli Naive Bayes model\n",
    "#from sklearn.naive_bayes import BernoulliNB\n",
    "#bernoulli_classifier = BernoulliNB(binarize = 1.0)\n",
    "#bernoulli_classifier.fit(train_data[0:,:-1],train_data[0:,-1])\n",
    "#print(bernoulli_classifier.score(train_data[0:,:-1],train_data[0:,-1]))\n",
    "#bernoulli_classifier.score(test_data[0:,:-1],test_data[0:,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training an Gaussian Naive Bayes model\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#gaussian_classifier = GaussianNB()\n",
    "#gaussian_classifier.fit(train_data[0:,:-1],train_data[0:,-1])\n",
    "#print(gaussian_classifier.score(train_data[0:,:-1],train_data[0:,-1]))\n",
    "#print(gaussian_classifier.score(test_data[0:,:-1],test_data[0:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#train_data, test_data= train_test_split(data_enc, test_size=0.20)#, random_state=1)\n",
    "\n",
    "# Training an Multinomial Naive Bayes model\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#multinomial_classifier = MultinomialNB()\n",
    "#multinomial_classifier.fit(train_data[0:,:-1],train_data[0:,-1])\n",
    "#print(multinomial_classifier.score(train_data[0:,:-1],train_data[0:,-1]))\n",
    "#print(multinomial_classifier.score(test_data[0:,:-1],test_data[0:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logisticReg import logistic_regression\n",
    "# test out minimization function based on gradient descend\n",
    "#def fn(x):\n",
    "    #return x[0]**2 + x[1]**2-1\n",
    "#    return x[0]**2 + x[1]**2 +x[2]**2+ x[0]*2 + x[1]*8 \n",
    "#def gd(x):\n",
    "#    return np.array([x[0]*2 + 2, x[1]*2 + 8, x[2]*2])\n",
    "\n",
    "# Minimize fn, make sure the learning rate alpha is small (otherwise the method diverges); alpha in [0.1,0.8] suggested\n",
    "#print(logistic_regression.gd_minimize(logistic_regression,gd,[0,0,0],0.5,0.00001))\n",
    "#print(\"min value = \", fn([-1,-4,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "logreg = logistic_regression()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "for i in range(0,10):\n",
    "    train_data, test_data= train_test_split(data_enc, test_size=0.50, random_state=i)\n",
    "    logreg.fit(train_data[0:,:-1], train_data[0:,-1], 0.04)  # for lambd > 0.042 \n",
    "    print(logreg.score(test_data[0:,:-1], test_data[0:,-1]))\n",
    "#logreg.fit(data_enc[0:,:-1], data_enc[0:,-1],0)\n",
    "#for i in range(0,10):\n",
    "#    print(data_enc[i,-1])\n",
    "#    print(logreg.proba(data_enc[i,:-1]))\n",
    "    \n",
    "#print(logreg.E(logreg.coef()))\n",
    "#print(logreg.coef())\n",
    "#logreg.score(data_enc[0:,:-1], data_enc[0:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(test_data[0:,:-1], test_data[0:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#for i in range(0,10):\n",
    "#    train_data, test_data= train_test_split(data_enc, test_size=0.30, random_state=3*i)\n",
    "#    clf = LogisticRegression(C = 100, random_state=0, fit_intercept = False, solver='lbfgs', multi_class='ovr').fit(train_data[0:,:-1], train_data[0:,-1])\n",
    "#    print(clf.score(test_data[0:,:-1], test_data[0:,-1]))#data_enc[0:,:-1], data_enc[0:,-1])\n",
    "clf = LogisticRegression(C = 100, random_state=0, fit_intercept = False, solver='lbfgs', multi_class='ovr').fit(data_enc[0:,:-1], data_enc[0:,-1])\n",
    "for i in range(0,10):\n",
    "    print(data_enc[i,-1])\n",
    "    print(clf.predict_proba([data_enc[i,:-1]])[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
